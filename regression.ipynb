{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "## to visualize the plots download $housing.csv$ and this Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries required for the project\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem and look at the big picture\n",
    "1. Define the objective in business terms.\n",
    "2. How will your solution be used?\n",
    "3. What are the current solutions/workarounds (if any)?\n",
    "4. How should you frame this problem (supervised/unsupervised, online/offline,\n",
    "etc.)?\n",
    "5. How should performance be measured?\n",
    "6. Is the performance measure aligned with the business objective?\n",
    "7. What would be the minimum performance needed to reach the business objective?\n",
    "8. What are comparable problems? Can you reuse experience or tools?\n",
    "9. Is human expertise available?\n",
    "10. How would you solve the problem manually?\n",
    "11. List the assumptions you (or others) have made so far.\n",
    "12. Verify assumptions if possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example, let's pretend we are assigned the task of building a model of house prices, the \"hello world!\" of Machine Learning. For this project, we are assigned a dataset of housing prices in California.  \n",
    "\n",
    "#### The first question we need to ask ourselves is: what is the purpose of building this model (1.1)? Usually the model itself is not the end goal, but it will be used for business purposes. Knowing this an advance will save us the embarassement (and company time) of building a model that does not provide the insights the team was expecting / needing (1.2). This information will help us decide on some specifics of the model / project: for instance, based on the company necessity, this could be an online model that continuosly recieves data an has to be monitored and re-trained on an hourly basis or an offline model, that will be monitored and trained again when the performance drops under a set threshold (1.4). \n",
    "In this case, since we are dealing with labeled data, we have a *supervised learning* task. It is a *regression* problem,since we are trying to make a prediction, and since we are dealing with multiple features, it is a *multivariate regression*. Now that we framed the problem we can choose a performance metric (1.5). Typical metrics for regression problems are RMSE (root mean squared error) and MAE (mean average error). RMSE indicates the standard deviation from the mean. If the data follows a normal distribution, an RMSE of 50k dollars means about 68% of the model predictions fall within 50k dollars of the actual value and 95% within $100k of the actual value. This metric is not robust against outliers: in the presence of many outliers it is preferrable to use MAE.\n",
    "\n",
    "#### Sharing information with the team and consulting human experts (1.9,1.10) with precious knowledge on the field which could save us time when building our model (for instance establishing a column of our dataset does not provide useful information to our model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data\n",
    "    \n",
    "1. List the data you need and how much you need.\n",
    "2. Find and document where you can get that data.\n",
    "3. Check how much space it will take.\n",
    "4. Check legal obligations, and get authorization if necessary.\n",
    "5. Get access authorizations.\n",
    "6. Create a workspace.\n",
    "7. Get the data.\n",
    "8. Convert the data to a format you can easily manipulate.\n",
    "9. Ensure sensitive information is deleted or protected.\n",
    "10. Sample a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is getting the data and preparing our workspace. Depennding on the nature of the data, we may need to make particular arrangements (2.3-5).\n",
    "\n",
    "#### Before starting to manipulate the data, we need to make sure we have all the proper software installed. It is good practice to setup an environment if working on several models: this will allow on using different version of the same Python library on the same machine.\n",
    "\n",
    "#### Finally, we can get the data (2.7). It is better to write custom functions and automate the process when possible (after the model is deployed, most of the time we need to periodically input more data to re train the model when its performance drops or to prevent it from dropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have the data we need to transform it in a format we can easily manipulate(2.8), like a *Pandas* dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = pd.read_csv('housing.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last step is to sample a test set and put it aside. We will pick some instances randomly, let's say 20% of the dataset. One way of doing so is by using a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(hd, 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shortcoming of such function is that if we use it again, it will generate a different set of random indices, hence two different datasets. We can overcome this problem using a sklearn fucntion called *train_test_split* and setting a *random_state*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ran_train_set, ran_test_set = train_test_split(hd, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In theory, a random split is enough to generate two different dataset. In practice, there is a big risk our sample is biased e.g. not all the categories are well represented. To solve this problem, we should have a look at the type of data we have in each column and their distributions. We can check the data types using either *.dtypes* or *.info*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n"
    }
   ],
   "source": [
    "# the info method will also show us null values\n",
    "hd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have null values only in the *total_bedrooms* features, we will deal with this later. All features are numerical, except *ocean_proximity*. We can explore the numerical data with *.describe* and the categorical with the *.value_counts* method. Also, by looking at the names of the columns, we now know our label column is *median_house_value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "hd['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          longitude      latitude  housing_median_age   total_rooms  \\\ncount  20640.000000  20640.000000        20640.000000  20640.000000   \nmean    -119.569704     35.631861           28.639486   2635.763081   \nstd        2.003532      2.135952           12.585558   2181.615252   \nmin     -124.350000     32.540000            1.000000      2.000000   \n25%     -121.800000     33.930000           18.000000   1447.750000   \n50%     -118.490000     34.260000           29.000000   2127.000000   \n75%     -118.010000     37.710000           37.000000   3148.000000   \nmax     -114.310000     41.950000           52.000000  39320.000000   \n\n       total_bedrooms    population    households  median_income  \\\ncount    20433.000000  20640.000000  20640.000000   20640.000000   \nmean       537.870553   1425.476744    499.539680       3.870671   \nstd        421.385070   1132.462122    382.329753       1.899822   \nmin          1.000000      3.000000      1.000000       0.499900   \n25%        296.000000    787.000000    280.000000       2.563400   \n50%        435.000000   1166.000000    409.000000       3.534800   \n75%        647.000000   1725.000000    605.000000       4.743250   \nmax       6445.000000  35682.000000   6082.000000      15.000100   \n\n       median_house_value  \ncount        20640.000000  \nmean        206855.816909  \nstd         115395.615874  \nmin          14999.000000  \n25%         119600.000000  \n50%         179700.000000  \n75%         264725.000000  \nmax         500001.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "hd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems like *median_income* is not expressed in USD. Moreover, all the values are between 0.5 and 15. In this case we would contact the team that collect the data to understand why is this the case. For instance, in this dataset the *median_income* was capped and then scaled. \n",
    "#### We can also plot a histogram of our numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "hd.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the histograms we learned that:\n",
    "1. both $housing-median-age$ and $median-house-value$ where capped. This could be a problem, especially since the latter is our label: the model may learn prices never go above that limit. In a real working environment, we would have to check with the team that will use the results of our model if this is problem or not\n",
    "2. the features are in very different scales, we will need to deal with this later \n",
    "3. many histograms are *heavy-tailed*. Some machine learning algorithms require we transform these features into a more bell-shaped distribution using a log-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have a better understanding of the data, we may decide to use a different technique for the train and test split, for instance we can do so that all the categorical variables in *ocean_proximity* are proportioannly represented. \n",
    "\n",
    "#### Let's pretend we had the opportunity to talk to an expert and found out that the median income is an important feature to rpedict median house prices. We want to make sure all the various categories of incomes are well represented in the test set. We can do so by creating a categorical attribute for the income, for the sole purpose of doing a *stratified* split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd[\"income_cat\"] = np.ceil(hd[\"median_income\"] / 1.5)\n",
    "hd[\"income_cat\"].where(hd[\"income_cat\"] < 5, 5.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3.0    35.058140\n2.0    31.884690\n4.0    17.630814\n5.0    11.443798\n1.0     3.982558\nName: income_cat, dtype: float64"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# we succesfully created a categorical variable with 5 categories\n",
    "hd['income_cat'].value_counts()/len(hd)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3.0    35.368217\n2.0    31.589147\n4.0    17.732558\n5.0    11.409884\n1.0     3.900194\nName: income_cat, dtype: float64"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "#compare with random split\n",
    "ran_train_set, ran_test_set = train_test_split(hd, test_size=0.2, random_state=15)\n",
    "ran_test_set['income_cat'].value_counts()/len(ran_test_set)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(hd, hd[\"income_cat\"]):\n",
    "    train_set = hd.loc[train_index]\n",
    "    test_set = hd.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Overall %  Stratified %   Random %  Stratified % error  Random % error\n3.0  35.058140     35.053295  35.368217            0.013820       -0.884467\n2.0  31.884690     31.879845  31.589147            0.015195        0.926911\n4.0  17.630814     17.635659  17.732558           -0.027480       -0.577082\n5.0  11.443798     11.458333  11.409884           -0.127011        0.296359\n1.0   3.982558      3.972868   3.900194            0.243309        2.068127",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Overall %</th>\n      <th>Stratified %</th>\n      <th>Random %</th>\n      <th>Stratified % error</th>\n      <th>Random % error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3.0</th>\n      <td>35.058140</td>\n      <td>35.053295</td>\n      <td>35.368217</td>\n      <td>0.013820</td>\n      <td>-0.884467</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>31.884690</td>\n      <td>31.879845</td>\n      <td>31.589147</td>\n      <td>0.015195</td>\n      <td>0.926911</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>17.630814</td>\n      <td>17.635659</td>\n      <td>17.732558</td>\n      <td>-0.027480</td>\n      <td>-0.577082</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>11.443798</td>\n      <td>11.458333</td>\n      <td>11.409884</td>\n      <td>-0.127011</td>\n      <td>0.296359</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>3.982558</td>\n      <td>3.972868</td>\n      <td>3.900194</td>\n      <td>0.243309</td>\n      <td>2.068127</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "metrics = pd.DataFrame()\n",
    "metrics['Overall %'] = hd['income_cat'].value_counts()/len(hd)*100\n",
    "metrics['Stratified %'] = test_set['income_cat'].value_counts()/len(test_set)*100\n",
    "metrics['Random %'] = ran_test_set['income_cat'].value_counts()/len(ran_test_set)*100\n",
    "metrics['Stratified % error'] = 100-(metrics['Stratified %']/metrics['Overall %']*100) \n",
    "metrics['Random % error'] = 100-(metrics['Random %']/metrics['Overall %']*100) \n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see from the above table that the stratified split gives us a better representation of all the categories, compared to the random split. Before moving foward we only need to remove the *income-cat* column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in (train_set, test_set):\n",
    "    col.drop([\"income_cat\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the data to gain insights\n",
    "\n",
    "1. Create a copy of the data for exploration (sampling it down to a manageable size\n",
    "if necessary).\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration.\n",
    "3. Study each attribute and its characteristics:\n",
    "• Name\n",
    "• Type (categorical, int/float, bounded/unbounded, text, structured, etc.)• % of missing values\n",
    "• Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "• Possibly useful for the task?\n",
    "• Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    "4. For supervised learning tasks, identify the target attribute(s).\n",
    "5. Visualize the data.\n",
    "6. Study the correlations between attributes.\n",
    "7. Study how you would solve the problem manually.\n",
    "8. Identify the promising transformations you may want to apply.\n",
    "9. Identify extra data that would be useful.\n",
    "10. Document what you have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When working on real projects, often the datasets are in the size of gigabytes. This can make the taks of exploring the data slow and tedious. An alternative is sampling the dataset (3.1). In this case we will just make a copy of the dataset to explore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 16512 entries, 17606 to 15775\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           16512 non-null  float64\n 1   latitude            16512 non-null  float64\n 2   housing_median_age  16512 non-null  float64\n 3   total_rooms         16512 non-null  float64\n 4   total_bedrooms      16354 non-null  float64\n 5   population          16512 non-null  float64\n 6   households          16512 non-null  float64\n 7   median_income       16512 non-null  float64\n 8   median_house_value  16512 non-null  float64\n 9   ocean_proximity     16512 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.4+ MB\n"
    }
   ],
   "source": [
    "df = train_set\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have the cooridnates of the districts with their housing prices, we can visualize the data using a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='latitude', y='longitude', figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can improve the quality of the plot using *transparency, size and color*. Having semi-transparent points will make the areas with higher density of data darker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='latitude', y='longitude', figsize=(12,12), alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While the alpha value has to be a number, we can pass a column to both *size & colour*. In the next example, the size of the points will reflect the population, and the color the median house price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='latitude', y='longitude', figsize=(12,12),alpha=0.4, s=df['population']/100,c='median_house_value',cmap=plt.get_cmap('jet'),colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we study the correlations(3.6). The *corr()* function computes the standard correlation coefficient between all atttributes. This can help us find redundant features and with features has a linear correlation with the label. This is computationally expensive, so in the case of big datasets the matrix can be calculated only after sampling. The correlation coefficients are still relevant, provided that the sampled dataset was stratified correctly. We also need to be aware that a weak or absent linear correlation does not mean a non-linear correlation does not exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "median_house_value    1.000000\nmedian_income         0.687160\ntotal_rooms           0.135097\nhousing_median_age    0.114110\nhouseholds            0.064506\ntotal_bedrooms        0.047689\npopulation           -0.026920\nlongitude            -0.047432\nlatitude             -0.142724\nName: median_house_value, dtype: float64"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "c_matrix = df.corr()\n",
    "c_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can visually spot correlations by simply plotting all the columns against each other. Since we have 9 columns we would end up with 81 plots, but in this case we will only focus on the 3 features most correlated with the label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_plot=['median_house_value','median_income','total_rooms','housing_median_age']\n",
    "scatter_matrix(df[corr_plot],figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is definitely a strong correlation between *median_income* & *median_house_value*. We also notice the price cap we saw before, along with some other few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter',x='median_income',y='median_house_value', figsize=(12,12), alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We may have to remove those lines (districts) to prevent the model from learning this weird patterns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also apply some data transformations. If we look at the feature again, we have the total population and total number of bedrooms in a district:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have the number of households, we can engineer some more useful features, like population per household , total rooms per household and bedrooms/rooms ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "median_house_value    1.000000\nmedian_income         0.687160\nrooms                 0.146285\ntotal_rooms           0.135097\nhousing_median_age    0.114110\nhouseholds            0.064506\ntotal_bedrooms        0.047689\npeople               -0.021985\npopulation           -0.026920\nlongitude            -0.047432\nlatitude             -0.142724\nratio                -0.259984\nName: median_house_value, dtype: float64"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "df_ = df.copy()\n",
    "df_['rooms'] = df_['total_rooms']/df_['households']\n",
    "df_['people'] = df_['population']/df_['households']\n",
    "df_['ratio'] = df_['total_bedrooms']/df_['total_rooms']\n",
    "c_matrix = df_.corr()\n",
    "c_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The new feature *ratio* has the second strongest correlation with our label. Wuth these information we can move with more confidence to the next step. Note that often times this is not the end of the data exploration regarding to the project. We may decide to come back to this step after we completed our model and analysed our output, in order to gain more insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare the Data\n",
    "\n",
    "1. Data cleaning: fix or remove outliers (optional), fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).\n",
    "2. Feature selection (optional): Drop the attributes that provide no useful information for the task.\n",
    "3. Feature engineering, where appropriate: discretize continuous features, decompose features (e.g., categorical, date/time, etc.)\n",
    "4. Feature scaling: standardize or normalize features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's time to prepare the data. We will write custom functions to apply the transformations. This will allow us:\n",
    "1. automatically transform other dataset (e.g. the test data we set aside and new training data in the future)\n",
    "2. re-use the functions in other projects\n",
    "3. treat the transformations as hyperparameters\n",
    "\n",
    "#### From what we noticed so far, we need to:\n",
    "\n",
    "1. deal with the missing values (we had them in $total-bedrooms$)\n",
    "1. deal with the categorical feature $ocean-proximity$\n",
    "1. add custom features\n",
    "1. deal with the features different scales\n",
    "1. use the log of the features with heavy-tailed distribution\n",
    "1. deal with the lines in the $median-income$/$median-house-value$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 16512 entries, 17606 to 15775\nData columns (total 9 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           16512 non-null  float64\n 1   latitude            16512 non-null  float64\n 2   housing_median_age  16512 non-null  float64\n 3   total_rooms         16512 non-null  float64\n 4   total_bedrooms      16354 non-null  float64\n 5   population          16512 non-null  float64\n 6   households          16512 non-null  float64\n 7   median_income       16512 non-null  float64\n 8   ocean_proximity     16512 non-null  object \ndtypes: float64(8), object(1)\nmemory usage: 1.3+ MB\n"
    }
   ],
   "source": [
    "df = train_set.drop(\"median_house_value\", axis=1)\n",
    "df_labels = train_set[\"median_house_value\"].copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have 4 options for the missing values:\n",
    "1. drop all the rows with missing values with *dropna*\n",
    "2. drop all the columns with missing values with *drop*\n",
    "3. set the missing values to some value (e.g. the median) using *Scikit-Learn Imputer* \n",
    "4. stratified the dataset and apply method 3 to each subset\n",
    "\n",
    "#### We will proceed with method n.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "df_num = df.drop('ocean_proximity',axis=1)\n",
    "imputer.fit(df_num)\n",
    "imputer.statistics_\n",
    "X = imputer.transform(df_num)\n",
    "df_tr = pd.DataFrame(X, columns=df_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16512 entries, 0 to 16511\nData columns (total 8 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           16512 non-null  float64\n 1   latitude            16512 non-null  float64\n 2   housing_median_age  16512 non-null  float64\n 3   total_rooms         16512 non-null  float64\n 4   total_bedrooms      16512 non-null  float64\n 5   population          16512 non-null  float64\n 6   households          16512 non-null  float64\n 7   median_income       16512 non-null  float64\ndtypes: float64(8)\nmemory usage: 1.0 MB\n"
    }
   ],
   "source": [
    "df_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. deal with the categorical feature $ocean-proximity$\n",
    "\n",
    "#### One approcah is using a label encoder, which will convert every unique category into a number. In this case we cannot use it, as machine learning models will imply there is an order in the numbers, e.g. category 5 is closer to category 4 then it is to category 1, which is not the case. A better approch will be using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1],\n       ...,\n       [0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0]])"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "oh = LabelBinarizer()\n",
    "df_cat = df[['ocean_proximity']]\n",
    "df_oh = oh.fit_transform(df_cat)\n",
    "df_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. add custom features\n",
    "#### For this step we will need to create a class. We can use the creation of bedrooms / rooms ratio as an hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "class Adder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ratio = True): \n",
    "        self.ratio = ratio\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms = X[:, rooms_ix] / X[:, household_ix]\n",
    "        people = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.ratio:\n",
    "            ratio = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms, people, ratio]\n",
    "        else:\n",
    "            return np.c_[X, rooms, people]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16512 entries, 0 to 16511\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       16512 non-null  float64\n 1   1       16512 non-null  float64\n 2   2       16512 non-null  float64\n 3   3       16512 non-null  float64\n 4   4       16512 non-null  float64\n 5   5       16512 non-null  float64\n 6   6       16512 non-null  float64\n 7   7       16512 non-null  float64\n 8   8       16512 non-null  float64\n 9   9       16512 non-null  float64\n 10  10      16512 non-null  float64\ndtypes: float64(11)\nmemory usage: 1.4 MB\n"
    }
   ],
   "source": [
    "attr_adder = Adder(ratio=True)\n",
    "tr_extra = attr_adder.transform(df_tr.values)\n",
    "df_tr_extra =  pd.DataFrame(tr_extra)\n",
    "df_tr_extra.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The latest dataframe has no missing values, because before creating the new features we filled the missing values in the original features using *Imputer*. The next step is feature scaling. We decided to use *standardization*, which is more robust against the outliers. We will integrate it in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_preprocess = Pipeline([\n",
    "('missing_values', SimpleImputer(strategy=\"median\")),\n",
    "('feature_eng', Adder(ratio=True)),\n",
    "('standardization', StandardScaler()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_tr = num_preprocess.fit_transform(df_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will ignore point 5 and 6 for now (use the log of the features with heavy-tailed distribution, deal with the lines in the *median-income/median-house-value*). We can return on these later if we notice our model is not performing well. The last step is merging the one-hot encoding process in the pipeline. We will use *FeatureUnion*. Since we are working with two parts of the dataframe separately, we need to define a method to select the part we want to pass to each pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_ = list(df_num)\n",
    "cat_ = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_preprocess, num_),\n",
    "        (\"cat\", OneHotEncoder(), cat_),\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = full_pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Short-List Promising Models\n",
    "\n",
    "1. Train many quick and dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "2. Measure and compare their performance (for each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds)\n",
    "3. Analyze the most significant variables for each algorithm.\n",
    "4. Analyze the types of errors the models make (what data would a human have used to avoid these errors?)\n",
    "5. Have a quick round of feature selection and engineering.\n",
    "6. Have one or two more quick iterations of the five previous steps.\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the training dataset is ready, it is time to train our first models and short-list the best ones. We will start with multivariate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "68628.19819848923"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "rg = LinearRegression()\n",
    "rg.fit(df_ready, df_labels)\n",
    "rg_pred = rg.predict(df_ready) \n",
    "rg_mse = mean_squared_error(df_labels,rg_pred)\n",
    "rg_rmse = np.sqrt(rg_mse)\n",
    "rg_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This means that the typical prediciton error is around 68k dollars, which is not great since most of the *median_house_values* range between 120k and 256k. This means the model is underfitting the training data. We can try to:\n",
    "1. Use a more powerful model\n",
    "1. feed the model better features (e.g. we could try the log of the heavy-tailed features)\n",
    "1. remove regularisation\n",
    "\n",
    "#### Since we haven't regularised the model yet we have to rule out number 3. Before going back to feature engineering (option 2), we can try a different model, like decision tree, SVM and random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "#decision tree regressor\n",
    "dec_tr = DecisionTreeRegressor()\n",
    "dec_tr.fit(df_ready,df_labels)\n",
    "tr_pred = dec_tr.predict(df_ready)\n",
    "tr_mse = mean_squared_error(df_labels,tr_pred)\n",
    "tr_rmse = np.sqrt(tr_mse)\n",
    "tr_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "114998.48948921208"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "# support vector machine regressor\n",
    "svr = SVR(kernel='rbf', C=100, gamma=1)\n",
    "svr.fit(df_ready,df_labels)\n",
    "svr_pred = svr.predict(df_ready)\n",
    "svr_mse = mean_squared_error(df_labels,svr_pred)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine-tune your models and combine them into a great solution\n",
    "\n",
    "1. Fine-tune the hyperparameters using cross-validation: treat your data transformation choices as hyperparameters, unless there are very few hyperparameter values to explore, prefer random search over grid search. \n",
    "2. Try Ensemble methods. \n",
    "3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The decision tree has zero error, which means the model is so good that fits all the training data, but it's not very likely to perform well on unseen data (overfitting). Since we cannot validat the result on the test set (yet) we can use a part of the training set to perform cross validation. In this case we will perform *K-fold cross validation*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_scores = cross_val_score(dec_tr, df_ready, df_labels, scoring = 'neg_mean_squared_error', cv=10)\n",
    "tr_rmse_score = np.sqrt(-tr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print('scores: ', scores)\n",
    "    print('scores average: ', scores.mean())\n",
    "    print('scores std dev: ', scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "scores:  [69144.15440697 67254.9325584  69668.17230368 68876.35818139\n 69322.86772155 74377.35882996 70117.72010672 71788.84524801\n 75381.91425275 70185.38177443]\nscores average:  70611.77053838593\nscores std dev:  2402.7252609932557\n"
    }
   ],
   "source": [
    "display_scores(tr_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can apply cross validation to the linear regression too, in order to compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "scores:  [66782.73843989 66960.118071   70347.95244419 74739.57052552\n 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n 71552.91566558 67665.10082067]\nscores average:  69052.46136345083\nscores std dev:  2731.6740017983425\n"
    }
   ],
   "source": [
    "lr_scores = cross_val_score(rg, df_ready, df_labels, scoring = 'neg_mean_squared_error', cv=10)\n",
    "lr_rmse_score = np.sqrt(-lr_scores)\n",
    "display_scores(lr_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So the decision tree is actually not performing as well as linear regression. Let's move to the next model: SVM. SVM does not seeem to be performing well, but we can try using different combination of the hyperparameters through a grid search. In this case we are trying a very small number of hyperparameters, but when the hyperparameters space becomes very large it is best to use a random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(cv=5, error_score='raise-deprecating',\n             estimator=SVR(C=100, cache_size=200, coef0=0.0, degree=3,\n                           epsilon=0.1, gamma=1, kernel='rbf', max_iter=-1,\n                           shrinking=True, tol=0.001, verbose=False),\n             iid='warn', n_jobs=None,\n             param_grid=[{'C': [30, 100, 300], 'gamma': [0.3, 1, 3, 10],\n                          'kernel': ['rbf']}],\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring='neg_mean_squared_error', verbose=0)"
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "svm_param = [\n",
    "    {'kernel': ['rbf'], 'C': [30, 100, 300], 'gamma': [0.3,1,3,10]}\n",
    "    ]\n",
    "svm_grid = GridSearchCV(svr, svm_param, cv=5, scoring='neg_mean_squared_error')\n",
    "svm_grid.fit(df_ready,df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVR(C=300, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.3,\n    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "114592.76534069762 {'C': 30, 'gamma': 0.3, 'kernel': 'rbf'}\n117945.46989557479 {'C': 30, 'gamma': 1, 'kernel': 'rbf'}\n118773.67820979048 {'C': 30, 'gamma': 3, 'kernel': 'rbf'}\n118914.60851572135 {'C': 30, 'gamma': 10, 'kernel': 'rbf'}\n106517.78628279532 {'C': 100, 'gamma': 0.3, 'kernel': 'rbf'}\n115842.43140426329 {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n118405.21796830831 {'C': 100, 'gamma': 3, 'kernel': 'rbf'}\n118888.28605400377 {'C': 100, 'gamma': 10, 'kernel': 'rbf'}\n95252.79103138024 {'C': 300, 'gamma': 0.3, 'kernel': 'rbf'}\n110716.22044353209 {'C': 300, 'gamma': 1, 'kernel': 'rbf'}\n117424.91641792165 {'C': 300, 'gamma': 3, 'kernel': 'rbf'}\n118788.61693816051 {'C': 300, 'gamma': 10, 'kernel': 'rbf'}\n"
    }
   ],
   "source": [
    "svm_res = svm_grid.cv_results_\n",
    "for mean,score in zip(svm_res['mean_test_score'],svm_res['params']):\n",
    "    print(np.sqrt(-mean),score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The grid search suggests the lower the *gamma* is, the better our error gets. We can try performing another grid search with lower values of *gamma*. We can also try using a different kernel, like 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm_param_2 = [\n",
    "    {'kernel': ['rbf'], 'C': [300,1000], 'gamma': [0.003,0.01,0.03,0.1]},\n",
    "    {'kernel': ['linear'], 'C': [30,100,300,1000]}\n",
    "    ]\n",
    "svm_grid_2 = GridSearchCV(svr, svm_param_2, cv=5, scoring='neg_mean_squared_error')\n",
    "svm_grid_2.fit(df_ready,df_labels)\n",
    "\n",
    "svm_res_2 = svm_grid_2.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=1,\n    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "svm_grid_2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "108767.83105615216 {'C': 300, 'gamma': 0.003, 'kernel': 'rbf'}\n95347.86986187777 {'C': 300, 'gamma': 0.01, 'kernel': 'rbf'}\n84641.68341091352 {'C': 300, 'gamma': 0.03, 'kernel': 'rbf'}\n84514.70619517785 {'C': 300, 'gamma': 0.1, 'kernel': 'rbf'}\n93145.98873720676 {'C': 1000, 'gamma': 0.003, 'kernel': 'rbf'}\n78423.96918256629 {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n72115.92421364697 {'C': 1000, 'gamma': 0.03, 'kernel': 'rbf'}\n71925.25004187765 {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n75448.84743960595 {'C': 30, 'kernel': 'linear'}\n71603.12196479437 {'C': 100, 'kernel': 'linear'}\n70703.95891598675 {'C': 300, 'kernel': 'linear'}\n70445.41077712944 {'C': 1000, 'kernel': 'linear'}\n"
    }
   ],
   "source": [
    "for mean,score in zip(svm_res_2['mean_test_score'],svm_res_2['params']):\n",
    "    print(np.sqrt(-mean),score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With this grid search, it does not seem we do better than linear regression. Let's try using grid search on the random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fr_param = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "fr = RandomForestRegressor()\n",
    "fr_grid = GridSearchCV(fr, fr_param, cv=5,scoring='neg_mean_squared_error')\n",
    "fr_grid.fit(df_ready, df_labels)\n",
    "\n",
    "fr_res = fr_grid.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "63936.32731940089 {'max_features': 2, 'n_estimators': 3}\n55457.80944226246 {'max_features': 2, 'n_estimators': 10}\n52806.23379242126 {'max_features': 2, 'n_estimators': 30}\n59961.1193768923 {'max_features': 4, 'n_estimators': 3}\n52881.54320070661 {'max_features': 4, 'n_estimators': 10}\n50798.6131072422 {'max_features': 4, 'n_estimators': 30}\n58282.29413841001 {'max_features': 6, 'n_estimators': 3}\n52624.200333226436 {'max_features': 6, 'n_estimators': 10}\n50144.98860348818 {'max_features': 6, 'n_estimators': 30}\n58426.608030497635 {'max_features': 8, 'n_estimators': 3}\n51637.837535344275 {'max_features': 8, 'n_estimators': 10}\n49997.87939056066 {'max_features': 8, 'n_estimators': 30}\n63470.44804226408 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54157.720743395934 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n59925.435380061885 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n52346.44930721333 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n59314.091068046844 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n51703.70046291037 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
    }
   ],
   "source": [
    "for mean,score in zip(fr_res['mean_test_score'],fr_res['params']):\n",
    "    print(np.sqrt(-mean),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features=8, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=30,\n                      n_jobs=None, oob_score=False, random_state=None,\n                      verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "fr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last thing left to do before testing our model is learn which features are most relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0.3674729601098822, 'median_income'),\n (0.16364037016063535, 'INLAND'),\n (0.11514070853074168, 'people'),\n (0.06479410509927182, 'longitude'),\n (0.06313077461661126, 'latitude'),\n (0.055183637284620765, 'ratio'),\n (0.0545696011320957, 'rooms'),\n (0.04524291906650208, 'housing_median_age'),\n (0.016292189026623168, 'population'),\n (0.015087006082926797, 'total_bedrooms'),\n (0.014782782897478797, 'total_rooms'),\n (0.014261401945697501, 'households'),\n (0.004309158393240756, '<1H OCEAN'),\n (0.003629324517849334, 'NEAR OCEAN'),\n (0.002352734588069074, 'NEAR BAY'),\n (0.000110326547753713, 'ISLAND')]"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "feat_imp_fr = fr_grid.best_estimator_.feature_importances_\n",
    "extra_attr = ['rooms','people','ratio']\n",
    "cat_one_hot = list(oh.classes_)\n",
    "attributes = num_ + extra_attr + cat_one_hot\n",
    "sorted(zip(feat_imp_fr,attributes),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a real project we would have the option to go back and remove some of the less relevant features, then train the model again and see if its performance improved (for instance, of the $ocean_proximity$ feature, only the *inland* category seems relevant). For the sake of this exercise, we would just assume this our final model and move to the last step: evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "48412.374859911215"
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "model = fr_grid.best_estimator_\n",
    "\n",
    "X_test = test_set.drop('median_house_value', axis = 1)\n",
    "y_test = test_set['median_house_value'].copy()\n",
    "\n",
    "X_test_ready = full_pipeline.transform(X_test)\n",
    "\n",
    "predictions = model.predict(X_test_ready)\n",
    "mse = mean_squared_error(y_test,predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually the test error is worse than the one on the validation set. It did not happen in this specific project, but when it happens it is important to not use this metric to tweak the hyperparameters until the test error improves, as this will only overfit the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present your solution\n",
    "\n",
    "1. Document what you have done.\n",
    "2. Create a nice presentation: make sure you highlight the big picture first.\n",
    "3. Explain why your solution achieves the business objective.\n",
    "4. Don’t forget to present interesting points you noticed along the way: describe what worked and what did not, list your assumptions and your system’s limitations.\n",
    "5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one predictor of housing prices”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch, monitor, and maintain your system\n",
    "\n",
    "1. Get your solution ready for production \n",
    "2. Write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. Beware of slow degradation\n",
    "3. Retrain your models on a regular basis on fresh data (automate as much as possible)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit7f7cc6b8303f427ea0d24b94d8e66e25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}